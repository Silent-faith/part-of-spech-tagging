{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC2-2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Part of Speech Tagging.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1AgAi3Fz3u-"
      },
      "source": [
        "# Parts-of-Speech Tagging (POS)\n",
        "\n",
        "Here we will do part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective...) to each word in an input text.  Tagging is difficult because some words can represent more than one part of speech at different times. They are  **Ambiguous**. Let's look at the following example: \n",
        "\n",
        "- The whole team played **well**. [adverb]\n",
        "- You are doing **well** for yourself. [adjective]\n",
        "- **Well**, this assignment took me forever to complete. [interjection]\n",
        "- The **well** is dry. [noun]\n",
        "- Tears were beginning to **well** in her eyes. [verb]\n",
        "\n",
        "Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will: \n",
        "\n",
        "- Learn how parts-of-speech tagging works\n",
        "- Compute the transition matrix A in a Hidden Markov Model\n",
        "- Compute the transition matrix B in a Hidden Markov Model\n",
        "- Compute the Viterbi algorithm \n",
        "- Compute the accuracy of your own model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9LdUjEflnTd"
      },
      "source": [
        "# Importing the Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s7lWZlXz3vF"
      },
      "source": [
        "# Importing packages and loading in the data set \n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import string"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYeLSIXw_U9I"
      },
      "source": [
        "# Preprocessing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3IN1A0W_PHr"
      },
      "source": [
        "# Punctuation characters\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# Morphology rules used to assign unknown word tokens\n",
        "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "\n",
        "def get_word_tag(line, vocab): \n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag\n",
        "    else:\n",
        "        word, tag = line.split()\n",
        "        if word not in vocab: \n",
        "            # Handle unknown words\n",
        "            word = assign_unk(word)\n",
        "        return word, tag\n",
        "    return None \n",
        "\n",
        "\n",
        "def preprocess(vocab, data_fp):\n",
        "    \"\"\"\n",
        "    Preprocess data\n",
        "    \"\"\"\n",
        "    orig = []\n",
        "    prep = []\n",
        "\n",
        "    # Read data\n",
        "    with open(data_fp, \"r\") as data_file:\n",
        "\n",
        "        for cnt, word in enumerate(data_file):\n",
        "\n",
        "            # End of sentence\n",
        "            if not word.split():\n",
        "                orig.append(word.strip())\n",
        "                word = \"--n--\"\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            # Handle unknown words\n",
        "            elif word.strip() not in vocab:\n",
        "                orig.append(word.strip())\n",
        "                word = assign_unk(word)\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                orig.append(word.strip())\n",
        "                prep.append(word.strip())\n",
        "\n",
        "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "    return orig, prep\n",
        "\n",
        "\n",
        "def assign_unk(tok):\n",
        "    \"\"\"\n",
        "    Assign unknown word tokens\n",
        "    \"\"\"\n",
        "    # Digits\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Punctuation\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Upper-case\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Nouns\n",
        "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Verbs\n",
        "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Adjectives\n",
        "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Adverbs\n",
        "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    return \"--unk--\"\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nQu0dgeApcv"
      },
      "source": [
        "# Loading the Data and pre processing it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kkcYOo7z3vI",
        "outputId": "6b08bee7-427b-4987-91cf-fd591ffb36f2"
      },
      "source": [
        "# load in the training corpus\n",
        "with open(\"WSJ_02-21.pos\", 'r') as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "print(f\"A few items of the training corpus list\")\n",
        "print(training_corpus[0:5])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few items of the training corpus list\n",
            "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H24XB-9kz3vK",
        "outputId": "f70bfb19-4f68-4bd7-937c-f85ae35d22a0"
      },
      "source": [
        "# read the vocabulary data, split by each line of text, and save the list\n",
        "with open(\"hmm_vocab.txt\", 'r') as f:\n",
        "    voc_l = f.read().split('\\n')\n",
        "\n",
        "print(\"A few items of the vocabulary list\")\n",
        "print(voc_l[0:50])\n",
        "print()\n",
        "print(\"A few items at the end of the vocabulary list\")\n",
        "print(voc_l[-50:])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few items of the vocabulary list\n",
            "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
            "\n",
            "A few items at the end of the vocabulary list\n",
            "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad_XPKCSz3vL",
        "outputId": "7a97b380-3b12-4d19-e953-926053a3bb7e"
      },
      "source": [
        "# vocab: dictionary that has the index of the corresponding words\n",
        "vocab = {} \n",
        "\n",
        "# Get the index of the corresponding words. \n",
        "for i, word in enumerate(sorted(voc_l)): \n",
        "    vocab[word] = i       \n",
        "    \n",
        "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
        "cnt = 0\n",
        "for k,v in vocab.items():\n",
        "    print(f\"{k}:{v}\")\n",
        "    cnt += 1\n",
        "    if cnt > 5:\n",
        "        break"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary dictionary, key is the word, value is a unique integer\n",
            ":0\n",
            "!:1\n",
            "#:2\n",
            "$:3\n",
            "%:4\n",
            "&:5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHXDZFSgz3vM",
        "outputId": "6a5b3204-b10a-41c0-b1e8-8149589fe5f6"
      },
      "source": [
        "# load in the test corpus\n",
        "with open(\"WSJ_24.pos\", 'r') as f:\n",
        "    y = f.readlines()\n",
        "\n",
        "print(\"A sample of the test corpus\")\n",
        "print(y[0:10])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A sample of the test corpus\n",
            "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5bGZ0jNz3vN",
        "outputId": "b8292852-b9a5-402a-a3a8-de66aa824dd0"
      },
      "source": [
        "#corpus without tags, preprocessed\n",
        "_, prep = preprocess(vocab, \"test.words\")     \n",
        "\n",
        "print('The length of the preprocessed test corpus: ', len(prep))\n",
        "print('This is a sample of the test_corpus: ')\n",
        "print(prep[0:10])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the preprocessed test corpus:  34199\n",
            "This is a sample of the test_corpus: \n",
            "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGQfFcL049Aq"
      },
      "source": [
        "# Creating the Dictionaries transition counts and emmision counts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWVxtQTpz3vQ"
      },
      "source": [
        "def create_dictionaries(training_corpus, vocab):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        training_corpus: a corpus where each line has a word followed by its tag.\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output: \n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
        "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize the dictionaries using defaultdict\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
        "    prev_tag = '--s--' \n",
        "    \n",
        "    # use 'i' to track the line number in the corpus\n",
        "    i = 0 \n",
        "    \n",
        "    # Each item in the training corpus contains a word and its POS tag\n",
        "    # Go through each word and its tag in the training corpus\n",
        "    for word_tag in training_corpus:\n",
        "        \n",
        "        # Increment the word_tag count\n",
        "        i += 1\n",
        "        \n",
        "        # Every 50,000 words, print the word count\n",
        "        if i % 50000 == 0:\n",
        "            print(f\"word count = {i}\")\n",
        "            \n",
        "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
        "        word, tag = get_word_tag(word_tag,vocab) \n",
        "        \n",
        "        # Increment the transition count for the previous word and tag\n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        \n",
        "        # Increment the emission count for the tag and word\n",
        "        emission_counts[(tag, word)] += 1\n",
        "\n",
        "        # Increment the tag count\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
        "        prev_tag = tag\n",
        "        \n",
        "    return emission_counts, transition_counts, tag_counts"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8of9b2kMz3vR",
        "outputId": "e45ed8bf-08e2-4a4a-c3e4-c385e704bcd3"
      },
      "source": [
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word count = 50000\n",
            "word count = 100000\n",
            "word count = 150000\n",
            "word count = 200000\n",
            "word count = 250000\n",
            "word count = 300000\n",
            "word count = 350000\n",
            "word count = 400000\n",
            "word count = 450000\n",
            "word count = 500000\n",
            "word count = 550000\n",
            "word count = 600000\n",
            "word count = 650000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5rxuXXZj_ix",
        "outputId": "4e7f65ac-c7e6-40e5-dbf8-7af6327c67bc"
      },
      "source": [
        "# get all the POS states\n",
        "states = sorted(tag_counts.keys())\n",
        "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
        "print(\"View these POS tags (states)\")\n",
        "print(states)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of POS tags (number of 'states'): 46\n",
            "View these POS tags (states)\n",
            "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlPy_Jr1z3vV"
      },
      "source": [
        "<a name='ex-02'></a>\n",
        "### Exercise 02\n",
        "\n",
        "**Instructions:** Implement `predict_pos` that computes the accuracy of your model. \n",
        "\n",
        "- This is a sample prediction method in which wee predict that POs which is more frequent \n",
        "- To assign a part of speech to a word, assign the most frequent POS for that word in the training set. \n",
        "- Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!\n",
        "- Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkAntoHxz3vW"
      },
      "source": [
        "def predict_pos(prep, y, emission_counts, vocab, states):\n",
        "    '''\n",
        "    Input: \n",
        "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
        "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
        "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "        states: a sorted list of all possible tags for this assignment\n",
        "    Output: \n",
        "        accuracy: Number of times you classified a word correctly\n",
        "    '''\n",
        "    \n",
        "    # Initialize the number of correct predictions to zero\n",
        "    num_correct = 0\n",
        "    \n",
        "    # Get the (tag, word) tuples, stored as a set\n",
        "    all_words = set(emission_counts.keys())\n",
        "    \n",
        "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
        "    total = len(y)\n",
        "    for word, y_tup in zip(prep, y): \n",
        "\n",
        "        # Split the (word, POS) string into a list of two items\n",
        "        y_tup_l = y_tup.split()\n",
        "        \n",
        "        # Verify that y_tup contain both word and POS\n",
        "        if len(y_tup_l) == 2:\n",
        "            \n",
        "            # Set the true POS label for this word\n",
        "            true_label = y_tup_l[1]\n",
        "\n",
        "        else:\n",
        "            # If the y_tup didn't contain word and POS, go to next word\n",
        "            continue\n",
        "    \n",
        "        count_final = 0\n",
        "        pos_final = ''\n",
        "        \n",
        "        # If the word is in the vocabulary...\n",
        "        if word in vocab:\n",
        "            for pos in states:\n",
        "                        \n",
        "                # define the key as the tuple containing the POS and word\n",
        "                key = (pos,word)\n",
        "\n",
        "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
        "                if key in emission_counts: # complete this line\n",
        "\n",
        "                # get the emission count of the (pos,word) tuple \n",
        "                    count = emission_counts[key]\n",
        "\n",
        "                    # keep track of the POS with the largest count\n",
        "                    if count>count_final: # complete this line\n",
        "\n",
        "                        # update the final count (largest count)\n",
        "                        count_final = count\n",
        "\n",
        "                        # update the final POS\n",
        "                        pos_final = pos\n",
        "\n",
        "            # If the final POS (with the largest count) matches the true POS:\n",
        "            if pos_final == true_label: # complete this line\n",
        "            \n",
        "                # Update the number of correct predictions\n",
        "                num_correct += 1\n",
        "            \n",
        "    accuracy = num_correct / total\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5R72xNaz3vW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8622bbc-f255-470d-d15f-4968ef361010"
      },
      "source": [
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of prediction using predict_pos is 0.8869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGhHUeQ9z3vX"
      },
      "source": [
        "\n",
        "```CPP\n",
        "Accuracy of prediction using predict_pos is 0.8889\n",
        "```\n",
        "\n",
        "88.9% is really good for this warm up exercise. With hidden markov models, you should be able to get **95% accuracy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dQM9uKHz3vX"
      },
      "source": [
        "<a name='2'></a>\n",
        "# Part 2: Hidden Markov Models for POS\n",
        "\n",
        "Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder\n",
        "- The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. \n",
        "- In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. \n",
        "- By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.\n",
        "\n",
        "The Markov Model contains a number of states and the probability of transition between those states. \n",
        "- In this case, the states are the parts-of-speech. \n",
        "- A Markov Model utilizes a transition matrix, `A`. \n",
        "- A Hidden Markov Model adds an observation or emission matrix `B` which describes the probability of a visible observation when we are in a particular state. \n",
        "- In this case, the emissions are the words in the corpus\n",
        "- The state, which is hidden, is the POS tag of that word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx_58s9kz3vY"
      },
      "source": [
        "\n",
        "\n",
        "**Implementing the `create_transition_matrix` below for all tags.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hHe9zKOz3vY"
      },
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    ''' \n",
        "    Input: \n",
        "        alpha: number used for smoothing\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        transition_counts: transition count for the previous word and tag\n",
        "    Output:\n",
        "        A: matrix of dimension (num_tags,num_tags)\n",
        "    '''\n",
        "    # Get a sorted list of unique POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # Count the number of unique POS tags\n",
        "    num_tags = len(all_tags)\n",
        "    \n",
        "    # Initialize the transition matrix 'A'\n",
        "    A = np.zeros((num_tags,num_tags))\n",
        "    \n",
        "    # Get the unique transition tuples (previous POS, current POS)\n",
        "    trans_keys = set(transition_counts.keys())\n",
        "    \n",
        "    \n",
        "    # Go through each row of the transition matrix A\n",
        "    for i in range(num_tags):\n",
        "        \n",
        "        # Go through each column of the transition matrix A\n",
        "        for j in range(num_tags):\n",
        "\n",
        "            # Initialize the count of the (prev POS, current POS) to zero\n",
        "            count = 0\n",
        "        \n",
        "            # Define the tuple (prev POS, current POS)\n",
        "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
        "            key = (all_tags[i],all_tags[j])\n",
        "\n",
        "            # Check if the (prev POS, current POS) tuple \n",
        "            # exists in the transition counts dictionaory\n",
        "            if key in transition_counts: #complete this line\n",
        "                \n",
        "                # Get count from the transition_counts dictionary \n",
        "                # for the (prev POS, current POS) tuple\n",
        "                count = transition_counts[key]\n",
        "                \n",
        "            # Get the count of the previous tag (index position i) from tag_counts\n",
        "            count_prev_tag = tag_counts[all_tags[i]]\n",
        "            \n",
        "            # Apply smoothing using count of the tuple, alpha, \n",
        "            # count of previous tag, alpha, and number of total tags\n",
        "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)\n",
        "\n",
        "    return A"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgm9vzdPz3vZ",
        "outputId": "c16aacec-6944-4e8a-8397-189501cc30fd"
      },
      "source": [
        "alpha = 0.001\n",
        "for i in range(46):\n",
        "    tag_counts.pop(i,None)\n",
        "\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "# Testing your function\n",
        "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
        "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
        "\n",
        "#print(\"View a subset of transition matrix A\")\n",
        "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
        "print(A_sub)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A at row 0, col 0: 0.000012042\n",
            "A at row 3, col 1: 0.1819\n",
            "              RBS            RP           SYM        TO            UH\n",
            "RBS  3.204656e-06  3.204656e-06  3.204656e-06  0.012822  3.204656e-06\n",
            "RP   5.408194e-07  5.413602e-04  5.408194e-07  0.051378  5.408194e-07\n",
            "SYM  2.853393e-05  2.853393e-05  2.853393e-05  0.000029  2.853393e-05\n",
            "TO   6.629119e-05  6.622496e-08  6.622496e-08  0.000066  6.629119e-05\n",
            "UH   1.407539e-05  1.407539e-05  1.407539e-05  0.056316  2.816485e-02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXjmYmLcz3va"
      },
      "source": [
        "**Implement the `create_emission_matrix` below that computes the `B` emission probabilities matrix.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtV2TXBz3va"
      },
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        alpha: tuning parameter used in smoothing \n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        B: a matrix of dimension (num_tags, len(vocab))\n",
        "    '''\n",
        "    \n",
        "    # get the number of POS tag\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Get a list of all POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # Get the total number of unique words in the vocabulary\n",
        "    num_words = len(vocab)\n",
        "    \n",
        "    # Initialize the emission matrix B with places for\n",
        "    # tags in the rows and words in the columns\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "    \n",
        "    # Get a set of all (POS, word) tuples \n",
        "    # from the keys of the emission_counts dictionary\n",
        "    emis_keys = set(list(emission_counts.keys()))\n",
        "    \n",
        "    # Go through each row (POS tags)\n",
        "    for i in range(num_tags): # complete this line\n",
        "        \n",
        "        # Go through each column (words)\n",
        "        for j in range(num_words): # complete this line\n",
        "\n",
        "            # Initialize the emission count for the (POS tag, word) to zero\n",
        "            count = 0\n",
        "                    \n",
        "            # Define the (POS tag, word) tuple for this row and column\n",
        "            key = (all_tags[i],vocab[j])\n",
        "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
        "            if key in emission_counts.keys(): # complete this line\n",
        "        \n",
        "                # Get the count of (POS tag, word) from the emission_counts d\n",
        "                count = emission_counts[key]\n",
        "                \n",
        "            # Get the count of the POS tag\n",
        "            count_tag = tag_counts[all_tags[i]]\n",
        "                \n",
        "            # Apply smoothing and store the smoothed value \n",
        "            # into the emission matrix B for this row and column\n",
        "            B[i,j] = (count + alpha) / (count_tag+ alpha*num_words)\n",
        "    return B"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqNVq3uQz3vb",
        "outputId": "8781a8c4-1aeb-4723-a59b-1bb0c5b2e774"
      },
      "source": [
        "# creating your emission probability matrix. this takes a few minutes to run. \n",
        "for i in range(46):\n",
        "    tag_counts.pop(i,None)\n",
        "\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
        "\n",
        "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
        "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
        "\n",
        "# Try viewing emissions for a few words in a sample dataframe\n",
        "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
        "\n",
        "# Get the integer ID for each word\n",
        "cols = [vocab[a] for a in cidx]\n",
        "\n",
        "# Choose POS tags to show in a sample dataframe\n",
        "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
        "\n",
        "# For each POS tag, get the row number from the 'states' list\n",
        "rows = [states.index(a) for a in rvals]\n",
        "\n",
        "# Get the emissions for the sample of words, and the sample of POS tags\n",
        "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
        "print(B_sub)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "View Matrix position at row 0, column 0: 0.000009365\n",
            "View Matrix position at row 3, column 1: 0.000001026\n",
            "              725      adroitly     engineers      promoted       synergy\n",
            "CD   1.241942e-04  4.138426e-08  4.138426e-08  4.138426e-08  4.138426e-08\n",
            "NN   1.112820e-08  1.112820e-08  1.112820e-08  1.112820e-08  3.339573e-05\n",
            "NNS  2.451835e-08  2.451835e-08  2.942447e-04  2.451835e-08  2.451835e-08\n",
            "VB   5.501223e-08  5.501223e-08  5.501223e-08  5.501223e-08  5.501223e-08\n",
            "RB   4.716363e-08  9.437443e-05  4.716363e-08  4.716363e-08  4.716363e-08\n",
            "RP   5.339664e-07  5.339664e-07  5.339664e-07  5.339664e-07  5.339664e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hGF7gkzz3vc"
      },
      "source": [
        "<a name='3'></a>\n",
        "# Part 3: Viterbi Algorithm and Dynamic Programming\n",
        "\n",
        "In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, `A` and `B` to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. \n",
        "\n",
        "* **Initialization** - In this part you initialize the `best_paths` and `best_probabilities` matrices that you will be populating in `feed_forward`.\n",
        "* **Feed forward** - At each step, you calculate the probability of each path happening and the best paths up to that point. \n",
        "* **Feed backward**: This allows you to find the best path with the highest probabilities. \n",
        "\n",
        "<a name='3.1'></a>\n",
        "## Part 3.1:  Initialization \n",
        "\n",
        "You will start by initializing two matrices of the same dimension. \n",
        "\n",
        "- best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\n",
        "\n",
        "- best_paths: A matrix that helps you trace through the best possible path in the corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1AnIfohz3vd"
      },
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: initialize\n",
        "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        states: a list of all possible parts-of-speech\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
        "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
        "        corpus: a sequence of words whose POS is to be identified in a list \n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
        "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
        "    '''\n",
        "    # Get the total number of unique POS tags\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Initialize best_probs matrix \n",
        "    # POS tags in the rows, number of words in the corpus as the columns\n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "    \n",
        "    # Initialize best_paths matrix\n",
        "    # POS tags in the rows, number of words in the corpus as columns\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "    \n",
        "    # Define the start token\n",
        "    s_idx = states.index(\"--s--\")\n",
        "    \n",
        "    # Go through each of the POS tags\n",
        "    for i in range(num_tags): # complete this line\n",
        "        \n",
        "        # Handle the special case when the transition from start token to POS tag i is zero\n",
        "        if A[s_idx,i] == 0: # complete this line\n",
        "            \n",
        "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
        "            best_probs[i,0] = float('-inf')\n",
        "        \n",
        "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
        "        else:\n",
        "            \n",
        "            # Initialize best_probs at POS tag 'i', column 0\n",
        "            # Check the formula in the instructions above\n",
        "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\n",
        "\n",
        "    return best_probs, best_paths"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ctXd3ioz3ve"
      },
      "source": [
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGojNGvwz3ve",
        "outputId": "41cc8d83-e741-4abc-a56e-0fbed5fac12e"
      },
      "source": [
        "# Test the function\n",
        "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
        "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best_probs[0,0]: -28.6884\n",
            "best_paths[2,3]: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uFWh7Tiz3vf"
      },
      "source": [
        " Implement the `viterbi_forward` algorithm and store the best_path and best_prob for every possible tag for each word in the matrices `best_probs` and `best_tags` using the pseudo code below.\n",
        "\n",
        "`for each word in the corpus\n",
        "\n",
        "    for each POS tag type that this word may be\n",
        "    \n",
        "        for POS tag type that the previous word could be\n",
        "        \n",
        "            compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n",
        "            \n",
        "            retain the highest probability computed for the current word\n",
        "            \n",
        "            set best_probs to this highest probability\n",
        "            \n",
        "            set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability `\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2oV2-bEz3vg"
      },
      "source": [
        "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        A, B: The transiton and emission matrices respectively\n",
        "        test_corpus: a list containing a preprocessed corpus\n",
        "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
        "    Output: \n",
        "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
        "    '''\n",
        "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    # Go through every word in the corpus starting from word 1\n",
        "    # Recall that word 0 was initialized in `initialize()`\n",
        "    for i in range(1, len(test_corpus)): \n",
        "        \n",
        "        # Print number of words processed, every 5000 words\n",
        "        if i % 5000 == 0:\n",
        "            print(\"Words processed: {:>8}\".format(i))\n",
        "            \n",
        "        # For each unique POS tag that the current word can be\n",
        "        for j in range(num_tags): # complete this line\n",
        "            \n",
        "            # Initialize best_prob for word i to negative infinity\n",
        "            best_prob_i =  float(\"-inf\")\n",
        "            \n",
        "            # Initialize best_path for current word i to None\n",
        "            best_path_i = None\n",
        "\n",
        "            # For each POS tag that the previous word can be:\n",
        "            for k in range(num_tags): # complete this line\n",
        "            \n",
        "                # Calculate the probability = \n",
        "                # best probs of POS tag k, previous word i-1 + \n",
        "                # log(prob of transition from POS k to POS j) + \n",
        "                # log(prob that emission of POS j is word i)\n",
        "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\n",
        "                # check if this path's probability is greater than\n",
        "                # the best probability up to and before this point\n",
        "                if prob > best_prob_i: # complete this line\n",
        "                    \n",
        "                    # Keep track of the best probability\n",
        "                    best_prob_i = prob\n",
        "                    \n",
        "                    # keep track of the POS tag of the previous word\n",
        "                    # that is part of the best path.  \n",
        "                    # Save the index (integer) associated with \n",
        "                    # that previous word's POS tag\n",
        "                    best_path_i = k\n",
        "\n",
        "            # Save the best probability for the \n",
        "            # given current word's POS tag\n",
        "            # and the position of the current word inside the corpus\n",
        "            best_probs[j,i] = best_prob_i\n",
        "            \n",
        "            # Save the unique integer ID of the previous POS tag\n",
        "            # into best_paths matrix, for the POS tag of the current word\n",
        "            # and the position of the current word inside the corpus.\n",
        "            best_paths[j,i] = best_path_i\n",
        "    return best_probs, best_paths"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo7jM_N4z3vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2338d0-3a87-4454-f7f2-164d90c9fd4d"
      },
      "source": [
        "# this will take a few minutes to run => processes ~ 30,000 words\n",
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words processed:     5000\n",
            "Words processed:    10000\n",
            "Words processed:    15000\n",
            "Words processed:    20000\n",
            "Words processed:    25000\n",
            "Words processed:    30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvB2OaIxz3vj"
      },
      "source": [
        "Implementing the `viterbi_backward` algorithm, which returns a list of predicted POS tags for each word in the corpus.\n",
        "\n",
        "\n",
        "\n",
        "**In Step 1:**       \n",
        "Loop through all the rows (POS tags) in the last entry of `best_probs` and find the row (POS tag) with the maximum value.\n",
        "Convert the unique integer ID to a tag (a string representation) using the dictionary `states`.  \n",
        "\n",
        "Referring to the three-word corpus described above:\n",
        "- `z[2] = 28`: For the word 'upward' at position 2 in the corpus, the POS tag ID is 28.  Store 28 in `z` at position 2.\n",
        "- states(28) is 'RB': The POS tag ID 28 refers to the POS tag 'RB'.\n",
        "- `pred[2] = 'RB'`: In array `pred`, store the POS tag for the word 'upward'.\n",
        "\n",
        "**In Step 2:**  \n",
        "- Starting at the last column of best_paths, use `best_probs` to find the most likely POS tag for the last word in the corpus.\n",
        "- Then use `best_paths` to find the most likely POS tag for the previous word. \n",
        "- Update the POS tag for each word in `z` and in `preds`.\n",
        "\n",
        "Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.  \n",
        "`z[1] = best_paths[z[2],2]`  \n",
        "\n",
        "The small test following the routine prints the last few words of the corpus and their states to aid in debug."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfpfut0Gz3vk"
      },
      "source": [
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "    '''\n",
        "    This function returns the best path.\n",
        "    \n",
        "    '''\n",
        "    # Get the number of words in the corpus\n",
        "    # which is also the number of columns in best_probs, best_paths\n",
        "    m = best_paths.shape[1] \n",
        "    \n",
        "    # Initialize array z, same length as the corpus\n",
        "    z = [None] * m\n",
        "    \n",
        "    # Get the number of unique POS tags\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    # Initialize the best probability for the last word\n",
        "    best_prob_for_last_word = float('-inf')\n",
        "    \n",
        "    # Initialize pred array, same length as corpus\n",
        "    pred = [None] * m\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    ## Step 1 ##\n",
        "    \n",
        "    # Go through each POS tag for the last word (last column of best_probs)\n",
        "    # in order to find the row (POS tag integer ID) \n",
        "    # with highest probability for the last word\n",
        "    for k in range(num_tags): # complete this line\n",
        "\n",
        "        # If the probability of POS tag at row k \n",
        "        # is better than the previosly best probability for the last word:\n",
        "        if best_probs[k,-1]>best_prob_for_last_word: # complete this line\n",
        "            \n",
        "            # Store the new best probability for the last word\n",
        "            best_prob_for_last_word = best_probs[k,-1]\n",
        "    \n",
        "            # Store the unique integer ID of the POS tag\n",
        "            # which is also the row number in best_probs\n",
        "            z[m - 1]=k\n",
        "            \n",
        "    # Convert the last word's predicted POS tag\n",
        "    # from its unique integer ID into the string representation\n",
        "    # using the 'states' dictionary\n",
        "    # store this in the 'pred' array for the last word\n",
        "    pred[m - 1] = states[k]\n",
        "    \n",
        "    ## Step 2 ##\n",
        "    # Find the best POS tags by walking backward through the best_paths\n",
        "    # From the last word in the corpus to the 0th word in the corpus\n",
        "    for i in range(len(corpus)-1, -1, -1): # complete this line\n",
        "        \n",
        "        # Retrieve the unique integer ID of\n",
        "        # the POS tag for the word at position 'i' in the corpus\n",
        "        pos_tag_for_word_i = best_paths[np.argmax(best_probs[:,i]),i]\n",
        "        \n",
        "        # In best_paths, go to the row representing the POS tag of word i\n",
        "        # and the column representing the word's position in the corpus\n",
        "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
        "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
        "        \n",
        "        # Get the previous word's POS tag in string form\n",
        "        # Use the 'states' dictionary, \n",
        "        # where the key is the unique integer ID of the POS tag,\n",
        "        # and the value is the string representation of that POS tag\n",
        "        pred[i - 1] = states[pos_tag_for_word_i]\n",
        "        \n",
        "     \n",
        "    return pred"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Z4uistz3vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2060110-d4f2-4e09-e143-6d1d983e8fbb"
      },
      "source": [
        "# Run and test your function\n",
        "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
        "m=len(pred)\n",
        "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
        "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction for pred[-7:m-1] is: \n",
            " ['see', 'them', 'here', 'with', 'us', '.'] \n",
            " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
            "\n",
            "The prediction for pred[0:8] is: \n",
            " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
            " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAZTp-q1z3vl"
      },
      "source": [
        "<a name='4'></a>\n",
        "# Part 4: Predicting on a data set\n",
        "\n",
        "Compute the accuracy of your prediction by comparing it with the true `y` labels. \n",
        "- `pred` is a list of predicted POS tags corresponding to the words of the `test_corpus`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryuYr-iAz3vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141a20c9-5ca9-499f-ab40-cea975726f64"
      },
      "source": [
        "print('The third word is:', prep[8])\n",
        "print('Your prediction is:', pred[5])\n",
        "print('Your corresponding label y is: ', y[5])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The third word is: several\n",
            "Your prediction is: VB\n",
            "Your corresponding label y is:  be\tVB\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_riWn2xz3vm"
      },
      "source": [
        "<a name='ex-08'></a>\n",
        "### Exercise 08\n",
        "\n",
        "Implement a function to compute the accuracy of the viterbi algorithm's POS tag predictions.\n",
        "- To split y into the word and its tag you can use `y.split()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYYmBKmSz3vm"
      },
      "source": [
        "def compute_accuracy(pred, y):\n",
        "    '''\n",
        "    Input: \n",
        "        pred: a list of the predicted parts-of-speech \n",
        "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
        "    Output: \n",
        "        \n",
        "    '''\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Zip together the prediction and the labels\n",
        "    for prediction, y in zip(pred, y):\n",
        "        # Split the label into the word and the POS tag\n",
        "        word_tag_tuple = y.split()\n",
        "        \n",
        "        # Check that there is actually a word and a tag\n",
        "        # no more and no less than 2 items\n",
        "        if len(word_tag_tuple)!=2: # complete this line\n",
        "            continue \n",
        "        # store the word and tag separately\n",
        "        word, tag = word_tag_tuple\n",
        "        # Check if the POS tag label matches the prediction\n",
        "        if prediction == tag: # complete this line\n",
        "            # count the number of times that the prediction\n",
        "            # and label match\n",
        "            num_correct += 1\n",
        "            \n",
        "        # keep track of the total number of examples (that have valid labels)\n",
        "        total += 1\n",
        "    return (num_correct/total)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De8osZg9z3vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a17557-5b99-4843-bc62-457c721377a1"
      },
      "source": [
        "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the Viterbi algorithm is 0.9502\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}